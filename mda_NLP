from nltk.collocations import *
import nltk
import pandas as pd
import numpy as np
import re
import nltk
import nltk, re, string
from nltk.corpus import stopwords
import nltk
import math
import string
from nltk.corpus import stopwords
from collections import Counter
from nltk.stem.porter import *
from nltk.corpus import stopwords
from collections import Counter
from nltk.stem.porter import*
import os
import sys
from sklearn import feature_extraction
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from collections import defaultdict
import math
import operator



# Read created dictionary
pattern=r'\w[\w\',-]*\w'
with open("/Users/shenzongqi/Desktop/PythonProject/Stevens/2022_Research/envir_words.txt",'r') as f:
    envir = [line.strip() for line in f]
envir = " ".join(envir)
envir=nltk.regexp_tokenize(envir.lower(), pattern)

### Read company 10k report

def clean_report(text):
#tokens=nltk.regexp_tokenize(text.lower(), pattern)
    stop_words = stopwords.words('english')
### Clean 10k Report data: (Lower, remove stop words)
    tokens = [token.strip() \
              for token in nltk.word_tokenize(text.lower()) \
              if token.strip() not in stop_words and \
              token.strip() not in string.punctuation]
    return tokens


# Tf_Idf matrix

# Function1
def loadDataSet(tokens):
    dataset = tokens
    classVec = [0, 1, 0, 1, 0, 1]  # class
    return dataset, classVec

# Function2
def feature_select(dataset):
    # Total word frequency statistics
    doc_frequency = defaultdict(int) #Record the number of occurrences of each word, which can be understood as a variable-length list, as long as you index it, it will automatically expand the column
    for file in dataset:
        for word in file:
            doc_frequency[word] += 1
    # Calculate the TF value of each word
    word_tf = {}  # Store the tf value of each word
    for i in doc_frequency:
        word_tf[i] = doc_frequency[i] / sum(doc_frequency.values()) #sum(doc.frequency.values)

    # Calculate the IDF value of each word
    doc_num = len(dataset)
    word_idf = {}  # Store the idf value of each word
    word_doc = defaultdict(int)  # Stores the number of documents that contain the word
    for word in doc_frequency:
        for file in dataset:
            if word in file:
                word_doc[word] += 1
    #
    for word in doc_frequency:
        word_idf[word] = math.log(doc_num / (word_doc[word] + 1))

    # Calculate the value of TF*IDF for each word
    word_tf_idf = {}
    for word in doc_frequency:
        word_tf_idf[word] = word_tf[word] * word_idf[word]

    # Sort dictionary by value from largest to smallest
    dict_feature_select = sorted(word_tf_idf.items(), key=operator.itemgetter(1), reverse=True)
    return dict_feature_select


#####Test
def Analysis_10k(text):
    tokens = clean_report(text)
    data_list, label_list = loadDataSet(tokens)
    features = feature_select(data_list)  # TF-IDF values for all words
    df = pd.DataFrame(features)
    Fin = sum(df[1] / len(df[1]))
    return Fin





# Loop all files in dictionary
import os
import codecs
path = "/Users/shenzongqi/Desktop/PythonProject/Stevens/2022_Research/Data/inputmda_01" #folder directory
files= os.listdir(path) #Get the names of all files in a folder
scores = []
for file in files: #Traverse folders
    position = path+'//'+ file #Construct an absolute path, "\\", one of '\' is an escape character
    print (position)
    with codecs.open(position, 'r', encoding='utf-8',
                     errors='ignore') as f:
        text = [line.strip() for line in f]
        text = text[0]
        score = Analysis_10k(text)
        score = (file[:-4],score)
    scores.append(score)
scores = pd.DataFrame(scores)
print (scores)
scores.columns = ["CIK","Score"]


scores.to_excel("/Users/shenzongqi/Desktop/PythonProject/Stevens/2022_Research/Data/scores.xls", index=False)
